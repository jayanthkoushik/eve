<!DOCTYPE html><html lang="en"><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Eve: A Gradient Based Optimization Method with Locally and Globally Adaptive Learning Rates</title><link rel="stylesheet" href="https://jkoushik.me/assets/main.css"><div class="container"><div class="row justify-content-center"><div class="col-12 col-sm-12 col-md-11 col-lg-9 col-xl-8"><div class="text-right"> <button class="btn btn-link" id="theme-switch" disabled> <span>dark theme</span> </button></div><h1 id="title">Eve: A Gradient Based Optimization Method with Locally and Globally Adaptive Learning Rates<br><small></small></h1><p><ul class="lead list-inline"><li class="list-inline-item"> Hiroaki Hayashi<sup class="sup-right">1,*</sup><li class="list-inline-item"> Jayanth Koushik<sup class="sup-right">1,*</sup><li class="list-inline-item"> Graham Neubig<sup class="sup-right">1</sup></ul><ul class="list-inline"><li class="list-inline-item"><sup class="sup-left">1</sup>Carnegie Mellon University</ul><p class="small"><sup class="sup-left">*</sup>Equal contribution<h1>Abstract</h1><p class="lead"> Adaptive gradient methods for stochastic optimization adjust the learning rate for each parameter locally. However, there is also a global learning rate which must be tuned in order to get the best performance. In this paper, we present a new algorithm that adapts the learning rate locally for each parameter separately, and also globally for all parameters together. Specifically, we modify Adam, a popular method for training deep learning models, with a coefficient that captures properties of the objective function. Empirically, we show that our method, which we call Eve, outperforms Adam and other popular methods in training deep neural networks, like convolutional neural networks for image classification, and recurrent neural networks for language tasks.<div></div><h1 id="sec:intro"><span class="header-section-number">1</span> Introduction</h1><p>Training deep neural networks is a challenging non-convex optimization problem. Stochastic Gradient Descent (<span class="abbr">SGD</span>) is a simple way to move towards local optima by following the negative (sub)gradient. However, vanilla <abbr title='Stochastic Gradient Descent'>SGD</abbr> is slow in achieving convergence for large-scale problems. One issue arises from the use of a global learning rate, which is difficult to set. To prevent the loss from “bouncing around” or diverging in directions with high curvature, the learning rate must be kept small. But this leads to slow progress in directions with low curvature. In many problems, the sparsity of gradients creates an additional challenge for <abbr title='Stochastic Gradient Descent'>SGD</abbr>. Some parameters might be used very infrequently, but still be very informative. Therefore, these parameters should be given a large learning rate when observed.<p>The issues highlighted above motivate the need for adaptive learning rates that are local to each parameter in the optimization problem. While there has been much work in this area, here we focus on the family of methods based on the Adagrad algorithm<span class="citation" data-cites="duchi2011adaptive"><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></span>. These methods maintain a separate learning rate for each parameter; and these local learning rates are made adaptive using some variation of the sum of squared gradients. Roughly speaking, if the gradients for some parameter have been large, its learning rate is reduced; and if the gradients have been small, the learning rate is increased. Variations of Adagrad such as RMSprop<span class="citation" data-cites="tieleman2012lecture"><a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></span>, Adadelta<span class="citation" data-cites="zeiler2012adadelta"><a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></span>, or Adam<span class="citation" data-cites="kingma2014adam"><a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></span> are, by far, the most popular alternatives to vanilla <abbr title='Stochastic Gradient Descent'>SGD</abbr> for training deep neural networks.<p>In addition to parameter-specific local learning rates, the adaptive methods described above also have a global learning rate which determines the overall step size. In many cases, this global learning rate is left at its default value; however, to get the best performance, it needs to be tuned, and also adapted throughout the training process. A common strategy is to decay the learning rate over time, which adds an additional hyperparameter, the decay strength, that needs to be chosen carefully.<p>In this work, we address the problem of adapting the global learning rate with a simple method that incorporates “feedback” from the objective function. Our algorithm, Eve, introduces a scalar coefficient <span class="math inline">\(d_t\)</span> which is used to adapt the global learning rate to be <span class="math inline">\(\alpha_t = \alpha_1 / d_t\)</span>, where <span class="math inline">\(\alpha_1\)</span> is the initial learning rate. <span class="math inline">\(d_t\)</span> depends on the history of stochastic objective function evaluations, and captures two properties of its behavior: variation in consecutive iterations and sub-optimality. Intuitively, high variation should reduce the learning rate and high sub-optimality should increase the learning rate. We specifically apply this idea to Adam<span class="citation" data-cites="kingma2014adam"><a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></span>, a popular method for training deep neural networks.<h1 id="sec:relwork"><span class="header-section-number">2</span> Related work</h1><p>Our work builds on recent advancements in gradient based optimization methods with locally adaptive learning rates. Notable members in this family are Adagrad<span class="citation" data-cites="duchi2011adaptive"><a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></span>, Adadelta<span class="citation" data-cites="zeiler2012adadelta"><a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></span>, RMSProp<span class="citation" data-cites="tieleman2012lecture"><a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a></span>, Adam (and Adamax)<span class="citation" data-cites="kingma2014adam"><a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a></span>. These methods adapt the learning rate using sum of squared gradients, an estimate of the uncentered second moment. Some of these methods also use momentum, or running averages instead of the raw gradient. Being first-order methods, they are simple to implement, and computationally efficient. In practice, they perform very well and are the methods of choice for training large neural networks.<p>As was discussed in the introduction, these methods have a global learning rate which is generally constant or annealed over time. Two popular decay algorithms are exponential decay which sets <span class="math inline">\(\alpha_t = \alpha_1\exp(-\gamma t)\)</span>, and <span class="math inline">\(1 / t\)</span> decay which sets <span class="math inline">\(\alpha_t = \alpha_1 / (1 + \gamma t)\)</span>. Here <span class="math inline">\(\gamma\)</span> is the decay strength, <span class="math inline">\(t\)</span> is iteration number, and <span class="math inline">\(\alpha_1\)</span> is the initial learning rate. For Adam, <span class="citation" data-cites="kingma2014adam">ibid.</span> suggest <span class="math inline">\(1 / \sqrt{t}\)</span> decay which sets <span class="math inline">\(\alpha_t = \alpha_1 / \sqrt{1 + \gamma t}\)</span>. We compare our proposed method with Adam using these decay algorithms. There are other heuristic scheduling algorithms used in practice like reducing the learning rate by some factor after every some number of iterations, or reducing the learning rate when the loss on a held out validation set stalls. <span class="citation" data-cites="smith2017cyclical">Smith<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a></span> proposes a schedule where the learning rate is varied cyclically between a range of values.<p><span class="citation" data-cites="schaul2013no">Schaul et al.<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a></span> take up the more ambitious goal of completely eliminating the learning rate parameter, with a second-order method that computes the diagonal Hessian using the bbprop algorithm<span class="citation" data-cites="lecun1998efficient"><a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a></span>. Note, however, that the method is only applicable under mean squared error loss. Finally, for convex optimization, <span class="citation" data-cites="polyak1987introduction">Polyak<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a></span> proposes a way to select step sizes for the subgradient method when the optimum is known. It can be shown that with steps of size <span class="math inline">\((f_t - f^\star) / \|g_t\|^2\)</span> (where <span class="math inline">\(g_t\)</span> is a subgradient), the subgradient method converges to the optimal value under some conditions. Our method also makes use of the optimal value <span class="math inline">\(f^\star\)</span> for adapting the global learning rate.<h1 id="sec:method"><span class="header-section-number">3</span> Method</h1><h2 id="sec:method_prelim"><span class="header-section-number">3.1</span> Preliminaries: Adam</h2><p>Since our method builds on top of the Adam algorithm, we begin with a brief description of the method. First, we establish the notation: let <span class="math inline">\(f(\theta)\)</span> be a stochastic objective function with parameters <span class="math inline">\(\theta\)</span>, and let <span class="math inline">\(\theta_t\)</span> be the value of the parameters after time <span class="math inline">\(t\)</span>. Let <span class="math inline">\(f_t = f(\theta_t)\)</span> and <span class="math inline">\(g_t = \nabla_{\theta} f(\theta_t)\)</span>. Adam maintains a running average of the gradient given by <span id="eq:m"><span class="math display">\[ m_t = \beta_1 m_{t-1} + (1 - \beta_1)g_t, \qquad(1)\]</span></span> with <span class="math inline">\(m_0 = 0\)</span>. A correction term is applied to remove the bias caused by initializing with 0. <span id="eq:mhat"><span class="math display">\[ \widehat{m}_t = m_t / (1 - \beta_1^t). \qquad(2)\]</span></span> <span class="math inline">\(\widehat{m}_t\)</span> is an unbiased estimate of the gradient’s first moment assuming stationarity (<span class="math inline">\(\mathbb{E}_{}\left[{\widehat{m}_t}\right] = \mathbb{E}_{}\left[{g_t}\right]\)</span>). A similar term is computed using the squared gradients: <span id="eq:vvhat"><span class="math display">\[ \begin{aligned} v_t &amp;= \beta_2 v_{t-1} + (1 - \beta_2)g_t^2. \\ \widehat{v}_t &amp;= v_t / (1 - \beta_2^t). \end{aligned} \qquad(3)\]</span></span> <span class="math inline">\(\widehat{v}_t\)</span> is an unbiased estimate of the gradient’s uncentered second moment (<span class="math inline">\(\mathbb{E}_{}\left[{\widehat{v}_t}\right] = \mathbb{E}_{}\left[{g_t^2}\right]\)</span>). Then, Adam updates parameters using the update equation <span id="eq:theta"><span class="math display">\[ \theta_{t+1} = \theta_t - \alpha_t\frac{\widehat{m}_t}{\sqrt{\widehat{v}_t} + \epsilon}. \qquad(4)\]</span></span><h2 id="sec:method_eve"><span class="header-section-number">3.2</span> Proposed method: Eve</h2><p>Our method is motivated by simple intuitive arguments. Let <span class="math inline">\(f\)</span> denote the stochastic objective function that needs to be minimized. Let <span class="math inline">\(f_t\)</span> be it’s value at iteration <span class="math inline">\(t\)</span>, and let <span class="math inline">\(f^\star\)</span> be its global minimum. First, consider the quantity <span class="math inline">\(|f_t - f_{t-1}|\)</span>. This captures the variability in the objective function i.e., how much the function is changing from one step to the other. If this quantity is large, any particular stochastic evaluation of <span class="math inline">\(f\)</span> should be given less “weight”, and the step taken based on it should be small. So we would like the learning rate to depend inversely on <span class="math inline">\(|f_t - f_{t-1}|\)</span>. Next, consider the quantity <span class="math inline">\(f_t - f^\star\)</span>, where <span class="math inline">\(f^\star\)</span> is the expected global minimum of <span class="math inline">\(f\)</span>. This is the sub-optimality i.e., it denotes how far we are from the minimum at time <span class="math inline">\(t\)</span>. Intuitively, far from the minimum, we should take big steps to make quick progress, and if we are close to the minimum, we should take small steps. Hence we would like the learning rate to depend directly on <span class="math inline">\(f_t - f^\star\)</span>. Putting these two together, our method scales the learning rate by <span class="math inline">\((f_t - f^\star) / |f_t - f_{t-1}|\)</span>: <span id="eq:alpha"><span class="math display">\[ \alpha_t = \frac{\alpha_1}{d_t} = \alpha_1\frac{f_t - f^\star}{|f_t - f_{t-1}|}. \qquad(5)\]</span></span> However, this simple method is not stable because once the loss increases, the increase of numerator in Equation <a href="#eq:alpha">5</a> directly increases the learning rate. This can result in the learning rate blowing up due to taking an even bigger step, causing the numerator to further increase, and so forth. To prevent this, we make modifications to stabilize the update rule by first replacing <span class="math inline">\(f_t\)</span> in the numerator of Equation <a href="#eq:alpha">5</a> with <span class="math inline">\(\min\{f_t, f_{t-1}\}\)</span>. This will reduce the chance of the vicious cycle mentioned above by keeping the numerator at the same value if the loss increases. In addition, we clip the term with a range parameterized by a constant <span class="math inline">\(c\)</span> to avoid extreme values. <span id="eq:dhat"><span class="math display">\[ \widehat{d}_t = \text{clip}(d_t, [1 / c, c]). \qquad(6)\]</span></span> Finally, for smoothness, we take a running average of the clipped <span class="math inline">\(d_t\)</span> <span id="eq:dtil"><span class="math display">\[ \widetilde{d}_t = \beta_3\widetilde{d}_{t-1} + (1 - \beta_3)\widehat{d}_t. \qquad(7)\]</span></span> The learning rate is then given by <span class="math inline">\(\alpha_t = \alpha_1 / \widetilde{d}_t\)</span>. Thus, the learning rate will be in the range <span class="math inline">\([\alpha_1 / c, c\alpha_1]\)</span>. Combining this with the Adam update rule gives our complete algorithm, which we call Eve, and is shown in Figure <a href="#fig:alg">1</a>. Below, the equations for computing <span class="math inline">\(\widetilde{d}_t\)</span> are summarized. We set <span class="math inline">\(\widetilde{d}_1 = 1\)</span>, and for <span class="math inline">\(t &gt; 1\)</span>, we have <span id="eq:summ"><span class="math display">\[ \begin{aligned} d_t &amp;= \frac{|f_t - f_{t-1}|}{\min\{f_t, f_{t-1}\} - f^\star}. \\ \widehat{d}_t &amp;= \text{clip}(d_t, [1 / c, c]). \\ \widetilde{d}_t &amp;= \beta_3\widetilde{d}_{t-1} + (1 - \beta_3)\widehat{d}_t. \end{aligned} \qquad(8)\]</span></span><figure> <img src="fig/alg.png" alt="Figure 1: Eve algorithm. Wherever applicable, products are element-wise." id="fig:alg" /><figcaption>Figure 1: Eve algorithm. Wherever applicable, products are element-wise.</figcaption></figure><h2 id="sec:method_limitations"><span class="header-section-number">3.3</span> Discussion of limitations</h2><p>One condition of our method, which can also be construed as a limitation, is that it requires knowledge of the global minimum of the objective function <span class="math inline">\(f^\star\)</span>. However, the method still remains applicable to a large class of problems. Particularly in deep learning, regularization is now commonly performed indirectly with dropout<span class="citation" data-cites="srivastava2014dropout"><a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a></span> or batch normalization<span class="citation" data-cites="ioffe2015batch"><a href="#fn15" class="footnote-ref" id="fnref15"><sup>15</sup></a></span> rather than weight decay. Therefore, under mean squared error or cross-entropy loss functions, the global minimum is simply 0. This will be the case for all our experiments, and we show that Eve can improve over other methods in optimizing complex, practical models.<h1 id="sec:exp"><span class="header-section-number">4</span> Experiments</h1><p>Now we conduct experiments to compare Eve with other popular optimizers used in deep learning. We use the same hyperparameter settings (as described in Figure <a href="#fig:alg">1</a>) for all experiments. We also conduct an experiment to study the behavior of Eve with respect to the new hyperparameters <span class="math inline">\(\beta_3\)</span> and <span class="math inline">\(c\)</span>. For each experiment, we use the same random number seed when comparing different methods. This ensures same weight initializations (we use the scheme proposed by <span class="citation" data-cites="glorot2010understanding">Glorot and Bengio<a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a></span> for all experiments), and mini-batch splits. In all experiments, we use cross-entropy as the loss function, and since the models don’t have explicit regularization, <span class="math inline">\(f^\star\)</span> is set to 0 for training Eve.<h2 id="sec:exp_cnns"><span class="header-section-number">4.1</span> Training CNNs</h2><p>First we compare Eve with other optimizers for training a Convolutional Neural Network (<span class="abbr">CNN</span>). The optimizers we compare against are Adam, Adamax, RMSprop, Adagrad, Adadelta, and <abbr title='Stochastic Gradient Descent'>SGD</abbr> with Nesterov momentum<span class="citation" data-cites="nesterov1983method"><a href="#fn17" class="footnote-ref" id="fnref17"><sup>17</sup></a></span> (momentum <span class="math inline">\(0.9\)</span>). The learning rate was searched over <span class="math inline">\(\{1\times 10^{-6}\)</span>, <span class="math inline">\(5\times 10^{-6}\)</span>, <span class="math inline">\(1\times 10^{-5}\)</span>, <span class="math inline">\(5\times 10^{-5}\)</span>, <span class="math inline">\(1\times 10^{-4}\)</span>, <span class="math inline">\(5\times 10^{-4}\)</span>, <span class="math inline">\(1\times 10^{-3}\)</span>, <span class="math inline">\(5\times 10^{-3}\)</span>, <span class="math inline">\(1\times 10^{-2}\)</span>, <span class="math inline">\(5\times 10^{-2}\)</span>, <span class="math inline">\(1\times 10^{-1}\}\)</span>, and the value which led to the lowest final loss was selected for reporting results. For Adagrad, Adamax, and Adadelta, we additionally searched over the prescribed default learning rates (<span class="math inline">\(10^{-2}\)</span>, <span class="math inline">\(2\times 10^{-3}\)</span>, and <span class="math inline">\(1\)</span> respectively).<p>The model is a deep residual network<span class="citation" data-cites="he2016deep"><a href="#fn18" class="footnote-ref" id="fnref18"><sup>18</sup></a></span> with 16 convolutional layers. The network is regularized with batch normalization and dropout, and contains about 680,000 parameters, making it representative of a practical model.<p>Figure <a href="#fig:trloss">2</a>(a) shows the results of training this model on the <abbr title='CIFAR 100'>CIFAR 100</abbr> dataset<span class="citation" data-cites="krizhevsky2009learning"><a href="#fn19" class="footnote-ref" id="fnref19"><sup>19</sup></a></span> for 100 epochs with a batch size of 128. We see that Eve outperforms all other algorithms by a large margin. It quickly surpasses other methods, and achieves a much lower final loss at the end of training.<figure> <img src="fig/trloss.png" alt="Figure 2: Training loss comparison. In both experiments, Eve achieves similar or lower loss than other optimizers." id="fig:trloss" /><figcaption>Figure 2: Training loss comparison. In both experiments, Eve achieves similar or lower loss than other optimizers.</figcaption></figure><h2 id="sec:exp_rnns"><span class="header-section-number">4.2</span> Training RNNs</h2><p>We also compare our method with other optimizers for training Recurrent Neural Networks (<span class="abbr">RNNs</span>). We use the same algorithms as in the previous experiment, and the learning rate search was conducted over the same set of values.<p>We construct a <abbr title='Recurrent Neural Network'>RNN</abbr> for character-level language modeling task on Penn Treebank (<span class="abbr">PTB</span>)<span class="citation" data-cites="marcus1993building"><a href="#fn20" class="footnote-ref" id="fnref20"><sup>20</sup></a></span>. Specifically, the model consists of a 2-layer character-level Gated Recurrent Unit (<span class="abbr">GRU</span>)<span class="citation" data-cites="chung2014empirical"><a href="#fn21" class="footnote-ref" id="fnref21"><sup>21</sup></a></span> with hidden layers of size 256, with 0.5 dropout between layers. The sequence length is fixed to 100 characters, and the vocabulary is kept at the original size.<p>The results for training this model are shown in Figure <a href="#fig:trloss">2</a>(b). Different optimizers performed similarly on this task, with Eve achieving slightly higher loss than Adam and Adamax.<h2 id="sec:exp_decay"><span class="header-section-number">4.3</span> Comparison with decay strategies</h2><p>We also empirically compare Eve with three common decay policies: exponential (<span class="math inline">\(\alpha_t = \alpha_1 \exp(-\gamma t)\)</span>), <span class="math inline">\(1/t\)</span> (<span class="math inline">\(\alpha_t = \alpha_1 / (1 + \gamma t)\)</span>), and <span class="math inline">\(1/\sqrt{t}\)</span> (<span class="math inline">\(\alpha_t = \alpha_1 / \sqrt{1 + \gamma t}\)</span>). We consider the same <abbr title='CIFAR 100'>CIFAR 100</abbr> classification task described in Section <a href="#sec:exp_cnns">4.1</a>, and use the same <abbr title='Convolutional Neural Network'>CNN</abbr> model. We applied the three decay policies to Adam, and tuned both the initial learning rate and decay strength. Learning rate was again searched over the same set as in the previous experiments.<p>For <span class="math inline">\(\gamma\)</span>, we searched over a different set of values for each of the decay policies, such that final learning rate after 100 epochs would be <span class="math inline">\(\alpha_1 / k\)</span> where <span class="math inline">\(k\)</span> is in <span class="math inline">\(\{1 \times 10^{4}\)</span>, <span class="math inline">\(5 \times 10^{3}\)</span>, <span class="math inline">\(1 \times 10^{3}\)</span>, <span class="math inline">\(5 \times 10^{2}\)</span>, <span class="math inline">\(1 \times 10^{2}\)</span>, <span class="math inline">\(5 \times 10^{1}\)</span>, <span class="math inline">\(1 \times 10^{1}\)</span>, <span class="math inline">\(5 \times 10^{0}\}\)</span>.<p>Figure <a href="#fig:compsched">3</a>(a) compares Eve with the best exponential decay, the best <span class="math inline">\(1/t\)</span> decay, and the best <span class="math inline">\(1/\sqrt{t}\)</span> decay applied to Adam. We see that using decay closes some of the gap between the two algorithms, but Eve still shows faster convergence. Moreover, using such a decay policy requires careful tuning of the decay strength. As seen in Figure <a href="#fig:compsched">3</a>(b), for different decay strengths, the performance of Adam can vary a lot. Eve can achieve similar or better performance without tuning an additional hyperparameter.<figure> <img src="fig/compsched.png" alt="Figure 3: Results of comparing Eve with learning rate decay strategies. Plot (a) shows the best results for Adam with different decays. The final loss values are similar to that of Eve, but Eve converges faster, and does not require the tuning of an additional parameter. This can be an important factor as shown in plot (b). For suboptimal decay strengths, the performance of Adam varies a lot." id="fig:compsched" /><figcaption>Figure 3: Results of comparing Eve with learning rate decay strategies. Plot (a) shows the best results for Adam with different decays. The final loss values are similar to that of Eve, but Eve converges faster, and does not require the tuning of an additional parameter. This can be an important factor as shown in plot (b). For suboptimal decay strengths, the performance of Adam varies a lot.</figcaption></figure><h2 id="sec:exp_hyp"><span class="header-section-number">4.4</span> Effect of hyperparameters</h2><p>In this experiment, we study the behavior of Eve with respect to the two hyperparameters introduced over Adam: <span class="math inline">\(\beta_3\)</span> and <span class="math inline">\(c\)</span>. We use the previously presented ResNet model on <abbr title='CIFAR 100'>CIFAR 100</abbr>, and a <abbr title='Recurrent Neural Network'>RNN</abbr> model trained for question answering, on question 14 (picked randomly) of the bAbI-10k dataset<span class="citation" data-cites="weston2015towards"><a href="#fn22" class="footnote-ref" id="fnref22"><sup>22</sup></a></span>. The question answering model composes two separate <abbr title='Gated Recurrent Unit'>GRUs</abbr> (with hidden layers of size 256 each) for question sentences, and story passages.<p>We trained the models using Eve with <span class="math inline">\(\beta_3\)</span> in <span class="math inline">\(\{0\)</span>, <span class="math inline">\(0.00001\)</span>, <span class="math inline">\(0.0001\)</span>, <span class="math inline">\(0.001\)</span>, <span class="math inline">\(0.01\)</span>, <span class="math inline">\(0.1\)</span>, <span class="math inline">\(0.3\)</span>, <span class="math inline">\(0.5\)</span>, <span class="math inline">\(0.7\)</span>, <span class="math inline">\(0.9\)</span>, <span class="math inline">\(0.99\)</span>, <span class="math inline">\(0.999\)</span>, <span class="math inline">\(0.9999\)</span>, <span class="math inline">\(0.99999\}\)</span>, and <span class="math inline">\(c\)</span> in <span class="math inline">\(\{2\)</span>, <span class="math inline">\(5\)</span>, <span class="math inline">\(10\)</span>, <span class="math inline">\(15\)</span>, <span class="math inline">\(20\)</span>, <span class="math inline">\(50\)</span>, <span class="math inline">\(100\}\)</span>. For each <span class="math inline">\((\beta_3, c)\)</span> pair, we picked the best learning rate from the same set of values used in previous experiments. We also used Adam with the best learning rate chosen from the same set as Eve.<p>Figure <a href="#fig:hypsearch">4</a> shows the loss curves for each hyperparameter pair, and that of Adam. The bold line in the figure is for <span class="math inline">\((\beta_3, c) = (0.999, 10)\)</span>, which are the default values. For these particular cases, we see that for almost all settings of the hyperparameters, Eve outperforms Adam, and the default values lead to performance close to the best. In general, for different models and/or tasks, not all hyperparameter settings lead to improved performance over Adam, and we did not observe any consistent trend in the performance across hyperparameters. However, the default values suggested in this paper consistently lead to good performance on a variety of tasks. We also note that the default hyperparameter values were not selected based on this experiment, but through an informal initial search using a smaller model.<figure> <img src="fig/hypsearch.png" alt="Figure 4: Loss curves for training with Adam and Eve (with different choices for the hyperparameters)." id="fig:hypsearch" /><figcaption>Figure 4: Loss curves for training with Adam and Eve (with different choices for the hyperparameters).</figcaption></figure><h1 id="conclusion-and-future-work"><span class="header-section-number">5</span> Conclusion and Future Work</h1><p>We proposed a new algorithm, Eve, for stochastic gradient-based optimization. Our work builds on adaptive methods which maintain a separate learning rate for each parameter, and adaptively tunes the global learning rate using feedback from the objective function. Our algorithm is simple to implement, and is efficient, both computationally, and in terms of memory.<p>Through experiments with <abbr title='Convolutional Neural Network'>CNNs</abbr> and <abbr title='Recurrent Neural Network'>RNNs</abbr>, we showed that Eve outperforms other state of the art optimizers in optimizing large neural network models. We also compared Eve with learning rate decay methods and showed that Eve can achieve similar or better performance with far less tuning. Finally, we studied the hyperparameters of Eve and saw that a range of choices leads to performance improvement over Adam.<p>One limitation of our method is that it requires knowledge of the global minimum of the objective function. One possible approach to address this issue is to use an estimate of the minimum, and update this estimate as training progresses. This approach has been used when using Polyak step sizes with the subgradient method.<div id="refs" class="references"><div id="ref-chung2014empirical"><p>Chung, Junyoung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. “Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.” <em>arXiv Preprint arXiv:1412.3555</em>, 2014.</div><div id="ref-duchi2011adaptive"><p>Duchi, John, Elad Hazan, and Yoram Singer. “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.” <em>Journal of Machine Learning Research</em> 12, no. Jul (2011): 2121–59.</div><div id="ref-glorot2010understanding"><p>Glorot, Xavier, and Yoshua Bengio. “Understanding the Difficulty of Training Deep Feedforward Neural Networks.” In <em>Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</em>, 249–56, 2010.</div><div id="ref-he2016deep"><p>He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. “Deep Residual Learning for Image Recognition.” In <em>Proceedings of the Ieee Conference on Computer Vision and Pattern Recognition</em>, 770–78, 2016.</div><div id="ref-ioffe2015batch"><p>Ioffe, Sergey, and Christian Szegedy. “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.” <em>arXiv Preprint arXiv:1502.03167</em>, 2015.</div><div id="ref-kingma2014adam"><p>Kingma, Diederik, and Jimmy Ba. “Adam: A Method for Stochastic Optimization.” <em>arXiv Preprint arXiv:1412.6980</em>, 2014.</div><div id="ref-krizhevsky2009learning"><p>Krizhevsky, Alex, and Geoffrey Hinton. “Learning Multiple Layers of Features from Tiny Images,” 2009.</div><div id="ref-lecun1998efficient"><p>LeCun, Yann, Léon Bottou, Genevieve B Orr, and Klaus-Robert Müller. “Efficient Backprop.” In <em>Neural Networks: Tricks of the Trade</em>, 9–50. Springer, 1998.</div><div id="ref-marcus1993building"><p>Marcus, Mitchell P, Mary Ann Marcinkiewicz, and Beatrice Santorini. “Building a Large Annotated Corpus of English: The Penn Treebank.” <em>Computational Linguistics</em> 19, no. 2 (1993): 313–30.</div><div id="ref-nesterov1983method"><p>Nesterov, Yurii. “A Method for Unconstrained Convex Minimization Problem with the Rate of Convergence O (1/K^ 2).” In <em>Doklady an Ussr</em>, 269:543–47, 1983.</div><div id="ref-polyak1987introduction"><p>Polyak, Boris T. “Introduction to Optimization. Translations Series in Mathematics and Engineering.” <em>Optimization Software</em>, 1987.</div><div id="ref-schaul2013no"><p>Schaul, Tom, Sixin Zhang, and Yann LeCun. “No More Pesky Learning Rates.” In <em>International Conference on Machine Learning</em>, 343–51, 2013.</div><div id="ref-smith2017cyclical"><p>Smith, Leslie N. “Cyclical Learning Rates for Training Neural Networks.” In <em>Applications of Computer Vision (Wacv), 2017 Ieee Winter Conference on</em>, 464–72. IEEE, 2017.</div><div id="ref-srivastava2014dropout"><p>Srivastava, Nitish, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. “Dropout: A Simple Way to Prevent Neural Networks from Overfitting.” <em>The Journal of Machine Learning Research</em> 15, no. 1 (2014): 1929–58.</div><div id="ref-tieleman2012lecture"><p>Tieleman, Tijmen, and Geoffrey Hinton. “Lecture 6.5-Rmsprop: Divide the Gradient by a Running Average of Its Recent Magnitude.” <em>COURSERA: Neural Networks for Machine Learning</em> 4, no. 2 (2012): 26–31.</div><div id="ref-weston2015towards"><p>Weston, Jason, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart van Merriënboer, Armand Joulin, and Tomas Mikolov. “Towards Ai-Complete Question Answering: A Set of Prerequisite Toy Tasks.” <em>arXiv Preprint arXiv:1502.05698</em>, 2015.</div><div id="ref-zeiler2012adadelta"><p>Zeiler, Matthew D. “ADADELTA: An Adaptive Learning Rate Method.” <em>arXiv Preprint arXiv:1212.5701</em>, 2012.</div></div><section class="footnotes"><hr /><ol><li id="fn1"><p>Duchi et al., “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization,” <em>Journal of Machine Learning Research</em> 12, no. Jul (2011): 2121–59.<a href="#fnref1" class="footnote-back">↩</a><li id="fn2"><p>Tieleman and Hinton, “Lecture 6.5-Rmsprop: Divide the Gradient by a Running Average of Its Recent Magnitude,” <em>COURSERA: Neural Networks for Machine Learning</em> 4, no. 2 (2012): 26–31.<a href="#fnref2" class="footnote-back">↩</a><li id="fn3"><p>Zeiler, “ADADELTA: An Adaptive Learning Rate Method,” <em>arXiv Preprint arXiv:1212.5701</em>, 2012.<a href="#fnref3" class="footnote-back">↩</a><li id="fn4"><p>Kingma and Ba, “Adam: A Method for Stochastic Optimization,” <em>arXiv Preprint arXiv:1412.6980</em>, 2014.<a href="#fnref4" class="footnote-back">↩</a><li id="fn5"><p>Ibid.<a href="#fnref5" class="footnote-back">↩</a><li id="fn6"><p>Duchi et al., “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization,” <em>Journal of Machine Learning Research</em> 12, no. Jul (2011): 2121–59.<a href="#fnref6" class="footnote-back">↩</a><li id="fn7"><p>Zeiler, “ADADELTA: An Adaptive Learning Rate Method,” <em>arXiv Preprint arXiv:1212.5701</em>, 2012.<a href="#fnref7" class="footnote-back">↩</a><li id="fn8"><p>Tieleman and Hinton, “Lecture 6.5-Rmsprop: Divide the Gradient by a Running Average of Its Recent Magnitude,” <em>COURSERA: Neural Networks for Machine Learning</em> 4, no. 2 (2012): 26–31.<a href="#fnref8" class="footnote-back">↩</a><li id="fn9"><p>Kingma and Ba, “Adam: A Method for Stochastic Optimization,” <em>arXiv Preprint arXiv:1412.6980</em>, 2014.<a href="#fnref9" class="footnote-back">↩</a><li id="fn10"><p>“Cyclical Learning Rates for Training Neural Networks,” in <em>Applications of Computer Vision (Wacv), 2017 Ieee Winter Conference on</em> (IEEE, 2017), 464–72.<a href="#fnref10" class="footnote-back">↩</a><li id="fn11"><p>“No More Pesky Learning Rates,” in <em>International Conference on Machine Learning</em>, 2013, 343–51.<a href="#fnref11" class="footnote-back">↩</a><li id="fn12"><p>LeCun et al., “Efficient Backprop,” in <em>Neural Networks: Tricks of the Trade</em> (Springer, 1998), 9–50.<a href="#fnref12" class="footnote-back">↩</a><li id="fn13"><p>“Introduction to Optimization. Translations Series in Mathematics and Engineering,” <em>Optimization Software</em>, 1987.<a href="#fnref13" class="footnote-back">↩</a><li id="fn14"><p>Srivastava et al., “Dropout: A Simple Way to Prevent Neural Networks from Overfitting,” <em>The Journal of Machine Learning Research</em> 15, no. 1 (2014): 1929–58.<a href="#fnref14" class="footnote-back">↩</a><li id="fn15"><p>Ioffe and Szegedy, “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,” <em>arXiv Preprint arXiv:1502.03167</em>, 2015.<a href="#fnref15" class="footnote-back">↩</a><li id="fn16"><p>“Understanding the Difficulty of Training Deep Feedforward Neural Networks,” in <em>Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</em>, 2010, 249–56.<a href="#fnref16" class="footnote-back">↩</a><li id="fn17"><p>Nesterov, “A Method for Unconstrained Convex Minimization Problem with the Rate of Convergence O (1/K^ 2),” in <em>Doklady an Ussr</em>, vol. 269, 1983, 543–47.<a href="#fnref17" class="footnote-back">↩</a><li id="fn18"><p>He et al., “Deep Residual Learning for Image Recognition,” in <em>Proceedings of the Ieee Conference on Computer Vision and Pattern Recognition</em>, 2016, 770–78.<a href="#fnref18" class="footnote-back">↩</a><li id="fn19"><p>Krizhevsky and Hinton, “Learning Multiple Layers of Features from Tiny Images,” 2009.<a href="#fnref19" class="footnote-back">↩</a><li id="fn20"><p>Marcus et al., “Building a Large Annotated Corpus of English: The Penn Treebank,” <em>Computational Linguistics</em> 19, no. 2 (1993): 313–30.<a href="#fnref20" class="footnote-back">↩</a><li id="fn21"><p>Chung et al., “Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling,” <em>arXiv Preprint arXiv:1412.3555</em>, 2014.<a href="#fnref21" class="footnote-back">↩</a><li id="fn22"><p>Weston et al., “Towards Ai-Complete Question Answering: A Set of Prerequisite Toy Tasks,” <em>arXiv Preprint arXiv:1502.05698</em>, 2015.<a href="#fnref22" class="footnote-back">↩</a></ol></section></div></div></div><script src="https://jkoushik.me/assets/main.js"></script>
